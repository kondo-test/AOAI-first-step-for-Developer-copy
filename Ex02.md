# 演習 2) Azure OpenAI Studio からの AI モデルの利用

この演習では前の演習でデプロイした Azure OpenAI サービスの言語モデル、画像生成モデルについて Azure OpenAI Studio のプレイグランド画面からの利用方法を体験します。埋め込みモデルについては、タスク 3 の[独自データの追加]()で使用します。

## 目次

この演習 2 で実施するタスクは以下のとおりです。

1. [チャット プレイグラウンドのパラメーター設定]()
2. [ブロンプト エンジニアリング ]()
3. [独自データの追加]()
4. [Azure OpenAI Studio で作成したチャットボットを Azure App Service にデプロイ]()\(オプション\)

## タスク 1 : システムメッセージとパラメーター設定

チャット プレイグラウンドの \[セットアップ\] パネルには GTP モデルが生成処理を行う際のパラメーターに紐づいた設定があります。このパラメーター設定を変更することで、生成されるテキストの内容を制御することができます。

![プレイグランドのセットアップパネル](images/chatPlayGround_setupPanel.png)


### システム メッセージ

システム メッセージは LLM（大規模言語モデル）の動作をガイドするためのメタプロンプトとも呼ばれ、AI に対してどのような応答を求めるかを指定します。これにより、モデルが特定のタスクを効果的に実行し、望ましい出力を生成するように導くことができます。

システムメッセージには以下のような要素が含まれることがあります：

1. **役割の定義**：モデルがどのような役割を果たすべきかを指定
    
    例：「カスタマーサポートエージェントとして行動する」

2. **タスクの説明**：モデルがどのようなタスクを実行するかを明確に

    例：「ユーザーの質問に答える」

3. **出力形式の指定**：モデルが生成する出力の形式やスタイルを指定

    例：「重要な部分を太字にする」

4. **安全ガイドライン**：モデルが生成してはいけない内容や、避けるべき行動を明示する

    例：「有害なコンテンツを生成しない」

これにより、モデルの応答が一貫性を持ち、ユーザーにとって有用で安全なものとなります。システムメッセージは、特定のシナリオやアプリケーションに応じてカスタマイズされることが多いです。

![Parameter description](images/OpenAI_gtp_systemmsg.png)


#### システムメッセージの動作の確認

システム メッセージを指定することで言語モデルのふるまいがどのように変化するかを確認します。具体的な手順は以下のとおりです。

\[手順\]

1. Azure OpenAI Studio にサインインし、ここまでの手順でデプロイした言語モデルのチャット プレイグラウンド画面を開きます

2. 言語モデルとのチャット画面で以下のメッセージを入力し、送信します

    ```
    やまたのおろち製作所について教えてください
    ```
    なお、「やまたのおろち製作所」は架空の企業名で、この地球上に存在しない企業です。

    以下のように、応答として生成された正しくない情報が混じることを確認します。

    ![ハルシネーション](images/gtp_hallucination.png)

    >このように言語モデルが事実に基づかない情報や誤った情報を生成する現象をハルシネーション(Hallucination)と言います

3. \[**システム メッセージ**\] タブの \[**システム メッセージ**\] のテキストボックスに以下の文言を入力し、\[**変更の適用**\] ボタンをクリックします

    ```
    あなたは誠実なアシスタントです。知らないことについては正直に「わかりません」と回答します。
    ```

    \[**システムメッセージを更新しますか?**\] と表示されるので、\[**続行**\] ボタンをクリックします

4. 再度、同じ質問を入力し、送信します

    ```
    やまたのおろち製作所について教えてください
    ```

5. 今度は回答に生成された情報が混じらないことを確認します

    ![システムメッセージ設定後の回答](images/gtp_no_hallucination.png)

システムメッセージによって言語モデルのふるまいが変化することが確認できました。

システムメッセージのより詳しい説明については以下のドキュメントご参照ください。

* [プロンプト エンジニアリングの技術 - システム メッセージ](https://learn.microsoft.com/ja-jp/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#system-message)

<br>    

### パラメーター

LLM（大規模言語モデル）におけるパラメーターはモデルの出力を制御するための設定です。

Azure OpenAI Studio のチャット プレイグランド画面では、以下のパラメーターを設定できます：

| パラメーター名         | 説明                                                                 |
|---|---|
| 過去のメッセージを含む | 新しい API 要求ごとに含める過去のメッセージの数を選択します。この数値を 10 に設定すると、5 つのユーザー クエリと 5 つのシステム応答が含まれます。                |
| 最大応答数 (max_tokens)| モデルが生成する応答の最大トークン(※)数を設定します。サポートされているトークンの数は、プロンプトとモデルの応答の間で共有されます。      |
| 温度 (temperature)| ランダム性を制御します。温度を下げることは、モデルがより反復的および決定論的な応答を生成することを意味します。温度を上げると、予期しない応答や創造的な応答が増えます。温度または上位 P の両方ではなくどちらかを調整してみてください。 |
| 上位 P (top_p)| 温度と同様にランダム性を制御しますが、別の方法を使用します。上位 P を下げると、モデルのトークンの選択がより可能性が高いトークンに絞り込まれます。上位 P を上げると、確率が高いトークンと低いトークンの両方からモデルが選択できるようになります。温度または上位 P の両方ではなくどちらかを調整してみてください。 |
| シーケンスの停止 (stop)| モデルが応答を生成する際に、特定のトークンやシーケンスで出力を停止するように設定します。      |
| 頻度のペナルティ (frequency_penalty)| モデルが同じトークンを繰り返し使用する頻度を減少させるためのペナルティを設定します。          |
| プレゼンスのペナルティ (presence_penalty)| モデルが新しいトピックやトークンを導入する頻度を増加させるためのペナルティを設定します。      |

![Parameter description](images/OpenAI_gtp_params.png)

> ※ **トークン :** LLM（大規模言語モデル）における「トークン」は、テキストデータを処理するための基本単位です。トークンは単語、部分的な単語、または文字のような小さな単位に分割されます。例えば、「cat」という単語は1つのトークンですが、「unbelievable」は「un」「believ」「able」のように複数のトークンに分割されることがあります。トークン化のプロセスは、モデルがテキストを理解し、生成する際に重要です。トークンの数は、モデルが処理するテキストの長さや複雑さに影響を与えます。トークンの数が多いほど、モデルの計算負荷が増加します。なお、このトークンの計算方法は言語やモデルによって異なり、実際の数は OpenAI 社が提供しているツール [**Tokenizer**](https://platform.openai.com/tokenizer) で確認できます。

## パラメーターの動作の確認

パラメーターの \[**過去のメッセージを含む**\] の値を変更し、言語モデルのふるまいがどのように変化するかを確認します。

具体的な手順は以下のとおりです。

\[手順\]

1. Azure OpenAI Studio にサインインし、ここまでの手順でデプロイした言語モデルのチャット プレイグラウンド画面を開きます

2. 言語モデルとのチャット画面で以下のメッセージをそれぞれ順に入力し、各回答が各質問に対するものであることを確認します

    ```plaintext
    Microsoft の設立日はいつですか?
    ```

    ```plaintext
    IBM の設立日はいつですか?
    ```

    ```plaintext
    Apple Computer の設立日はいつですか?
    ```

    ```plaintext
    Google の設立日はいつですか?
    ```

    ```plaintext
    Amazon の設立日はいつですか?
    ```

3. 以下のメッセージを送信し、言語モデルから回答された内容が、これまでに返された回答と矛盾がないことを確認します

    ```plaintext
    私がした 3 つ前の質問にある会社の創業者は誰ですか?
    ```

4. \[**パラメーター**\] タブの \[**過去のメッセージを含む**\] の内容を \[**3**\] に変更し、チャット画面の \[**チャットをクリアする**\] ボタンをクリックします

    ![過去のメッセージを含む パラメーターの変更](images/chatPlayGround_changePastMsg.png)

    「**チャットをクリアしますか?**」とメッセージボックスが表示されるので、\[**クリア**\] ボタンをクリックします

5. 再度、この手順 2 の質問を順に入力し、各回答が各質問に対するものであることを確認します

6. 以下のメッセージを送信し、言語モデルから回答された内容が、**これまでに返された回答の内容とあっていない**ことを確認します

    ```plaintext
    私がした 3 つ前の質問にある会社の創業者は誰ですか?
    ```

これは ChatGPT が会話において \[**過去のメッセージを含む**\] に設定された数だけ過去の会話を保持し、過去の質問に基づいて回答を生成することができることを示しています。

なお、この仕組みとメッセージの構造については、演習 3 のタスク 1 で詳しく説明します。

その他、パラメーターの詳細については以下のドキュメントをご参照ください。

* [**Azure OpenAI Service REST API reference**](https://learn.microsoft.com/ja-jp/azure/ai-services/openai/reference#request-body)

<br>

## タスク 2 : プロンプト エンジニアリング

言語モデルを使用するには、ユーザーはモデルに対し、人間と会話するように自然言語で命令を行います。この命令を含んだメッセージをプロンプトと言います。

プロンプト エンジニアリングは、モデルが生成するテキストの品質を向上させるための手法で、モデルに対して適切なプロンプトを提供することで、モデルが特定のタスクを効果的に実行し、望ましい出力を生成するように導くことができます。

**プロンプトの構造**

言語モデルに対するプロンプトは、以下の主要な要素で構成されます：

* **Instructions（指示）**: モデルに対して何をするかを指示する部分

    ex) 以下のプロンプトの場合、Instructions は「**以下の文を英語にしてください**:」の部分です。

    ```
    以下の文を英語にしてください:
    大使館への行き方を教えてください
    ```

* **Primary content**（主要コンテンツ）: モデルが処理または変換するテキスト

    ex) 以下のプロンプトの場合、Primary content は「**大使館への行き方を教えてください**」の部分です。

    ```
    以下の文を英語にしてください:
    大使館への行き方を教えてください
    ```

* **Examples**（例）: モデルに望ましい動作を示すための入力と出力のペア

    ex) 以下のプロンプトの場合、Examples は「**「べき等性」とは**」の部分です。

    ```
    以下の言葉を説明してください:
    「べき等性」とは
    ```
    
    言語モデルは "「べき等性」とは" に続く文章を生成することが期待されます。

* **Cues（キュー）**: モデルの出力を望ましい方向に導くための前置き


* **Supporting content**（補助コンテンツ）: 出力に影響を与えるための追加情報。




<!--
https://learn.microsoft.com/ja-jp/training/modules/apply-prompt-engineering-azure-openai/
-->